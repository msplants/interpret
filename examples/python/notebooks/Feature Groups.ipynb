{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Setup a regression experiment"]},{"cell_type":"code","execution_count":15,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\jwolk\\Miniconda3\\envs\\msplants-interpret\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning:\n","\n","Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n","\n","    The Boston housing prices dataset has an ethical problem. You can refer to\n","    the documentation of this function for further details.\n","\n","    The scikit-learn maintainers therefore strongly discourage the use of this\n","    dataset unless the purpose of the code is to study and educate about\n","    ethical issues in data science and machine learning.\n","\n","    In this special case, you can fetch the dataset from the original\n","    source::\n","\n","        import pandas as pd\n","        import numpy as np\n","\n","\n","        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n","        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n","        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n","        target = raw_df.values[1::2, 2]\n","\n","    Alternative datasets include the California housing dataset (i.e.\n","    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n","    dataset. You can load the datasets as follows::\n","\n","        from sklearn.datasets import fetch_california_housing\n","        housing = fetch_california_housing()\n","\n","    for the California housing dataset and::\n","\n","        from sklearn.datasets import fetch_openml\n","        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n","\n","    for the Ames housing dataset.\n","    \n","\n"]}],"source":["import pandas as pd\n","from sklearn.datasets import load_boston\n","from sklearn.model_selection import train_test_split\n","\n","boston = load_boston()\n","feature_names = list(boston.feature_names)\n","df = pd.DataFrame(boston.data, columns=feature_names)\n","df[\"target\"] = boston.target\n","# df = df.sample(frac=0.1, random_state=1)\n","train_cols = df.columns[0:-1]\n","label = df.columns[-1]\n","X = df[train_cols]\n","y = df[label]\n","\n","seed = 1\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=seed)"]},{"cell_type":"markdown","metadata":{},"source":["## Explore the dataset"]},{"cell_type":"code","execution_count":16,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<!-- http://127.0.0.1:7001/2324070516424/ -->\n","<iframe src=\"http://127.0.0.1:7001/2324070516424/\" width=100% height=800 frameBorder=\"0\"></iframe>"]},"metadata":{},"output_type":"display_data"}],"source":["from interpret import show\n","from interpret.data import Marginal\n","\n","marginal = Marginal().explain_data(X_train, y_train, name = 'Train Data')\n","show(marginal)"]},{"cell_type":"markdown","metadata":{},"source":["## Train the Explainable Boosting Machine (EBM)"]},{"cell_type":"code","execution_count":18,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["ExplainableBoostingRegressor(n_jobs=-1, random_state=1)"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["from interpret.glassbox import ExplainableBoostingRegressor\n","\n","ebm = ExplainableBoostingRegressor(random_state=seed, n_jobs=-1)\n","ebm.fit(X_train, y_train)   #Works on dataframes and numpy arrays"]},{"cell_type":"markdown","metadata":{},"source":["## Global Explanations: What the model learned overall (shows the top -15- most important features)"]},{"cell_type":"code","execution_count":21,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<!-- http://127.0.0.1:7001/2324130872968/ -->\n","<iframe src=\"http://127.0.0.1:7001/2324130872968/\" width=100% height=800 frameBorder=\"0\"></iframe>"]},"metadata":{},"output_type":"display_data"}],"source":["ebm_global = ebm.explain_global(name='EBM')\n","show(ebm_global)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from interpret.visual.plot import sort_take\n","\n","data_dict = sort_take(data_dict, sort_fn=lambda x: -abs(x), top_n=100, reverse_results=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Additional Code to Compute Feature Group Importances and Append Feature Group Importances to a Global Explanation"]},{"cell_type":"code","execution_count":22,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","\n","def get_feature_group_importance(feature_group, ebm, X, contributions=None):\n","    \"\"\"Computes the feature importance for a group of features\n"," \n","    Args:\n","        feature_group: A list of feature names\n","        ebm: A fitted EBM\n","        X: Numpy array of samples\n","        contributions (optional): Contributions of all features per row\n"," \n","    Returns:\n","        A double\n","    \"\"\"\n","    if contributions is None:\n","        _, contributions = ebm.predict_and_contrib(X)\n","    abs_sum_per_row = np.empty(len(contributions), np.float64)\n"," \n","    # For all rows in the dataset\n","    for i in range(len(contributions)):\n","        sum = 0.0\n","        # For all features in feature_group\n","        for j, feat_name in enumerate(ebm.get_feature_names_out()):\n","            if feat_name in feature_group:\n","                sum += contributions[i][j]\n","        abs_sum_per_row[i] = abs(sum)\n","    \n","    return np.average(abs_sum_per_row)\n"," \n"," \n","def get_group_and_individual_importances(feature_group, ebm, X):\n","    \"\"\"Utility function to compute the feature importance for a group \n","       of features as well as each feature in the group\n"," \n","    Args:\n","        feature_group: A list of feature names\n","        ebm: A fitted EBM\n","        X: Numpy array of samples\n"," \n","    Returns:\n","        A list of tuples, where each tuple is in the form (feature_name, importance)\n","    \"\"\"\n","    _, contributions = ebm.predict_and_contrib(X)\n","    importances = []\n"," \n","    for feature in feature_group:\n","         importances.append( (feature, get_feature_group_importance(feature, ebm, X, contributions)) )\n"," \n","    importances.append( (feature_group, get_feature_group_importance(feature_group, ebm, X, contributions)) )\n","    \n","    return importances\n"," \n","def append_feature_importance(feature_name, feature_importance, global_exp):\n","    \"\"\" Appends a feature name and importance to the global explanation, which\n","        will only be displayed in the \"Summary\" Graph\n"," \n","    Args:\n","        feature_name (string)\n","        feature_importance (double)\n","        global_exp: An EBM Global Explanation\n","    \"\"\"\n","    if global_exp._internal_obj is not None and global_exp._internal_obj[\"overall\"] is not None:\n","        global_exp._internal_obj[\"overall\"][\"names\"].append(feature_name)\n","        global_exp._internal_obj[\"overall\"][\"scores\"].append(feature_importance)\n","    else:\n","        print(\"It was not possible to append feature {} to the global explanation.\".format(feature_name))\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Add Feature Groups to Global Explanation "]},{"cell_type":"code","execution_count":23,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Importance for feature group ['RM', 'NOX']: 2.0728997603879087\n","Importance for feature group ['CHAS', 'ZN']: 0.41325989100551636\n"]}],"source":["ebm_global = ebm.explain_global(name='EBM')\n","\n","feature_group1 = [\"RM\", \"NOX\"]\n","fg1_importance = get_feature_group_importance(feature_group1, ebm, X)\n","print(\"Importance for feature group {}: {}\".format(feature_group1, fg1_importance))\n","\n","feature_group2 = [\"CHAS\", \"ZN\"]\n","fg2_importance = get_feature_group_importance(feature_group2, ebm, X)\n","print(\"Importance for feature group {}: {}\".format(feature_group2, fg2_importance))"]},{"cell_type":"code","execution_count":24,"metadata":{"trusted":true},"outputs":[],"source":["from interpret.visual.plot import sort_take\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["append_feature_importance(\"RM & NOX\", fg1_importance, ebm_global)\n","append_feature_importance(\"CHAS & ZN\", fg2_importance, ebm_global)\n","show(ebm_global)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"}},"nbformat":4,"nbformat_minor":4}
